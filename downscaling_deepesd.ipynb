{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5f008e-3c02-4d7e-8c18-d5f98acd84e2",
   "metadata": {},
   "source": [
    "## Downscaling with the DeepESD model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b09c8-7891-4085-89ca-6568c0c338f3",
   "metadata": {},
   "source": [
    "This notebook showcases a simple application of deep4downscaling for the statistical downscaling of precipitation. To do so, we will implement the following actions:\n",
    "\n",
    "- Define and train the DeepESD architecture [1].\n",
    "- Downscale and evaluate results over a test period.\n",
    "- Downscale outputs from a Global Climate Model (GCM).\n",
    "- Generate the corresponding downscaled climate change signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e39fa9-f444-4cf1-969d-f8183210046d",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0fece-5aca-4dda-b225-4d9312a94302",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/input'\n",
    "FIGURES_PATH = './figures'\n",
    "MODELS_PATH = './models'\n",
    "ASYM_PATH = './data/asym'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a7a51-719b-4ac7-9708-ac0e1fdc780c",
   "metadata": {},
   "source": [
    "When working with climate data, xarray is an essential library, and deep4downscaling heavily relies on it. For the deep learning component, deep4downscaling uses PyTorch, one of the most popular frameworks in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad4df3-e0a1-42b5-9809-406360f25b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import sys; sys.path.append('/home/jovyan/deep4downscaling')\n",
    "import deep4downscaling.viz\n",
    "import deep4downscaling.trans\n",
    "import deep4downscaling.deep.loss\n",
    "import deep4downscaling.deep.utils\n",
    "import deep4downscaling.deep.models\n",
    "import deep4downscaling.deep.train\n",
    "import deep4downscaling.deep.pred\n",
    "import deep4downscaling.metrics\n",
    "import deep4downscaling.metrics_ccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9b8b8-3878-439a-925f-fa4d66f58ad8",
   "metadata": {},
   "source": [
    "We will begin by loading the predictor. In this case, we select various large-scale variables from ERA5 at different height levels. These variables are already stored in a NetCDF file, the standard data format for deep4downscaling. Unfortunately, due to GitHub's size restrictions, we are unable to upload these files to the repository. However, the following cells provide an overview of the data, making it straightforward to reproduce this notebook with a similar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d8bb6-4937-49ad-a894-6df0419cbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictors\n",
    "predictor_filename = f'{DATA_PATH}/ERA5_NorthAtlanticRegion_1-5dg_full.nc'\n",
    "predictor = xr.open_dataset(predictor_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374a87a-b378-422c-b3f9-91da3af3dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79433a31-5b92-47a2-a9c1-3e9a7d60a515",
   "metadata": {},
   "source": [
    "The deep4downscaling library provides several functions to facilitate an initial visualization of the data. For example, the `deep4downscaling.viz.multiple_map_plot` function allows you to visualize an `xarray.Dataset`. These functions rely on matplotlib and cartopy. By default, the figure is saved as a `.pdf` file in the path specified by the `output_path argument`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b843ede-1388-410a-9ada-1ef0e21ad5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep4downscaling.viz.multiple_map_plot(data=predictor.mean('time'),\n",
    "                                       output_path=f'./{FIGURES_PATH}/predictor_climatology.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c2314-bf4e-456c-a4f5-22c9f2ca8fe6",
   "metadata": {},
   "source": [
    "The predictand is an `xarray.Dataset` containing a single variable (the target). In this notebook, we will focus on downscaling accumulated precipitation over the region of peninsular Spain and the Balearic Islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe9800-ae6b-4dbb-a611-db0f4c6e0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictand_filename = f'{DATA_PATH}/pr_AEMET.nc'\n",
    "predictand = xr.open_dataset(predictand_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568c642-6094-4c8a-a467-1d8e4e8e4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58c835-4f8c-4776-a951-26131d3988fb",
   "metadata": {},
   "source": [
    "Similar to the predictors, deep4downscaling can also be used for an initial visualization of the predictand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b0ffb-8c37-4204-81d9-43e43389b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_to_viz = '10-04-1981'\n",
    "deep4downscaling.viz.simple_map_plot(data=predictand.sel(time=day_to_viz),\n",
    "                                     colorbar='hot_r', var_to_plot='pr',\n",
    "                                     output_path=f'./{FIGURES_PATH}/predictand_day.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26de362-1f37-4c5b-932b-712e5bfc3c08",
   "metadata": {},
   "source": [
    "Deep4downscaling also includes several common preprocessing techniques used in statistical downscaling, such as removing NaN values, aligning datasets (e.g., across time), bias adjustment, and standardization, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711e216-1302-4bcf-b6af-5f7dae2443f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove days with nans in the predictor\n",
    "predictor = deep4downscaling.trans.remove_days_with_nans(predictor)\n",
    "\n",
    "# Align both datasets in time\n",
    "predictor, predictand = deep4downscaling.trans.align_datasets(predictor, predictand, 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749625d-0ec6-45d4-aad2-d64ba6856732",
   "metadata": {},
   "source": [
    "To adhere to the standard training/validation scheme in the machine learning field, we divide the predictors and predictand into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3fcbc8-365d-4b8c-bbbc-8d4ee88b1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_train = ('1980', '1981')\n",
    "years_test = ('1982', '1983')\n",
    "\n",
    "x_train = predictor.sel(time=slice(*years_train))\n",
    "y_train = predictand.sel(time=slice(*years_train))\n",
    "\n",
    "x_test = predictor.sel(time=slice(*years_test))\n",
    "y_test = predictand.sel(time=slice(*years_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1808f3-2f9d-490a-85d0-191091292b9c",
   "metadata": {},
   "source": [
    "Before feeding the predictors to the deep learning model, we standardize them to have a mean of zero and a standard deviation of one. This is done using the `deep4downscaling.trans.standardize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126affa4-b763-414f-8e06-679aa2e72c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stand = deep4downscaling.trans.standardize(data_ref=x_train, data=x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb58de7-1fb7-4ca4-9e30-ee90137c4630",
   "metadata": {},
   "source": [
    "For training and inference, the data will be transformed into the torch.Tensor type. To facilitate the transition from NetCDF to torch.Tensor, especially when computing projections (predictions), we define a mask around the predictand to use throughout the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf252d68-7a8e-49ce-ae1f-b121d1393b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mask = deep4downscaling.trans.compute_valid_mask(y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbf13f-e1ca-4d07-9b08-1c4bd4f4fedc",
   "metadata": {},
   "source": [
    "All deep learning models implemented in deep4downscaling flatten their output into a vector, standardizing its dimensions to the shape `(time, grid point)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb04bb7-430a-4eb4-bf61-9cf64e129226",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_stack = y_train.stack(gridpoint=('lat', 'lon'))\n",
    "y_mask_stack = y_mask.stack(gridpoint=('lat', 'lon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a345988-9910-46f0-b32b-1e887e66fa9f",
   "metadata": {},
   "source": [
    "The DeepESD architecture consists of a set of convolutional layers followed by a final dense layer. In our case, since the predictand contains NaN values for sea grid points, we filter out these grid points to save computation. This reduces the number of neurons in the final fully connected layer. By applying this operation using the mask, the conversion between the model's output and the corresponding NetCDF becomes straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a1c44-93ff-4e1c-b5f2-f94b3f6c5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mask_stack_filt = y_mask_stack.where(y_mask_stack==1, drop=True)\n",
    "y_train_stack_filt = y_train_stack.where(y_train_stack['gridpoint'] == y_mask_stack_filt['gridpoint'],\n",
    "                                             drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd021005-d6d2-452c-bc38-d30f60a712f9",
   "metadata": {},
   "source": [
    "The deep4downscaling library includes various loss functions for training deep learning models. In this notebook, we follow [2] and focus on the ASYmmetric loss function (ASYM). We have provided the values asym_weight=3 and cdf_weight=10 as an example of the flexibility of the loss function. Default values of asym_weight=1 and cdf_weight=2 are equivalent to the original loss.Asym at [3].Implementing custom loss functions should be straightforward, as they follow the typical PyTorch conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdc317-4af9-4b68-b6b8-b41cdb721cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = deep4downscaling.deep.loss.Asym(ignore_nans=True,\n",
    "                                                asym_path=ASYM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c2dbe-eb98-41fd-9e97-85a880bf93b2",
   "metadata": {},
   "source": [
    "For this loss function to work, we need to pre-compute a gamma distribution for each grid point in the predictand data (training set) on a yearly basis and calculate the mean of their parameters (see [3] for more details). This process can be handled by deep4downscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec297e2-03e7-498e-8885-5f3fb1efd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_function.parameters_exist():\n",
    "    loss_function.load_parameters()\n",
    "else:\n",
    "    loss_function.compute_parameters(data=y_train_stack_filt,\n",
    "                                     var_target='pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a91-6829-4483-bdb7-47c7b19c71d6",
   "metadata": {},
   "source": [
    "NetCDF is not well-suited for use with PyTorch (or for converting to the `torch.Tensor` type). In contrast, NumPy is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb3e96-b461-4afa-9d41-f427cf64ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stand_arr = deep4downscaling.trans.xarray_to_numpy(x_train_stand)\n",
    "y_train_arr = deep4downscaling.trans.xarray_to_numpy(y_train_stack_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21570ace-2e09-4254-96b0-d5b81d718ccc",
   "metadata": {},
   "source": [
    "With our data now in the numpy format, we can create the `torch.Dataset` and `torch.DataLoader` to feed batches of data to the deep learning model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa51b2-ec91-4f20-83a7-b3f4243206a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "train_dataset = deep4downscaling.deep.utils.StandardDataset(x=x_train_stand_arr,\n",
    "                                                            y=y_train_arr)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_dataset, valid_dataset = random_split(train_dataset,\n",
    "                                            [0.9, 0.1])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6dc5de-5aef-45ab-909a-74e3e587f507",
   "metadata": {},
   "source": [
    "Deep4downscaling includes several predefined deep learning architectures (e.g., DeepESD and U-Net), but custom architectures can be easily defined using the standard PyTorch framework. However, because deep4downscaling relies on a final flattening operation (as mentioned earlier), we recommend reviewing the implementations in `deep4downscaling.deep.models` and using them as a foundation.\n",
    "\n",
    "While deep4downscaling lacks a formal documentation page, all its functions and arguments are properly documented within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd4088-4d3a-49cd-b6d3-77fe8081153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "?deep4downscaling.deep.models.DeepESDpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba58ee-0864-4f90-a5b4-29ea97d4c7b5",
   "metadata": {},
   "source": [
    "In this notebook, we will train the DeepESD architecture with a single final convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115aa88-1fff-4afc-abc5-a9528996c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepesd_pr'\n",
    "model = deep4downscaling.deep.models.DeepESDpr(x_shape=x_train_stand_arr.shape,\n",
    "                                               y_shape=y_train_arr.shape,\n",
    "                                               filters_last_conv=1,\n",
    "                                               stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d683ac5-ad76-4573-837a-749b4c293140",
   "metadata": {},
   "source": [
    "We set the typical training hyperparameters, as is commonly done in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aa521-af2a-4ce7-8531-42b170074d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "patience_early_stopping = 20\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8dfe3-656b-4835-8487-7e6dc396a9f7",
   "metadata": {},
   "source": [
    "Deep learning models can run on either CPU or GPU devices. We provide the corresponding `.yml` environment files (`deep4downscaling/requirement`) to set up a basic Conda environment for running deep4downscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b4c50-d7fa-4f5d-869d-845a47118c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move ASYM paramters to device\n",
    "loss_function.prepare_parameters(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8dca3-2139-44f8-9c4e-6bb97fd7845a",
   "metadata": {},
   "source": [
    "Deep4downscaling provides the `deep4downscaling.deep.train.standard_training_loop`, which implements a basic training routine. Models are saved based on their performance on a validation set through an early stopping process, with the final saved model being the one that achieves the best score on this set. To disable early stopping, you can pass `None` to the `patience_early_stopping` argument. We recommend users consult the `?deep4downscaling.deep.train.standard_training_loop` for further details about this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9327b-2aca-4b53-9793-1e30bae6ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = deep4downscaling.deep.train.standard_training_loop(\n",
    "                            model=model, model_name=model_name, model_path=MODELS_PATH,\n",
    "                            device=device, num_epochs=num_epochs,\n",
    "                            loss_function=loss_function, optimizer=optimizer,\n",
    "                            train_data=train_dataloader, valid_data=valid_dataloader,\n",
    "                            patience_early_stopping=patience_early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0938e5a-c7ba-4b06-9b61-20afd933a2ad",
   "metadata": {},
   "source": [
    "### Downscale the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb291ec-3c08-4694-bb75-373240f23e32",
   "metadata": {},
   "source": [
    "Once a model has been trained and saved as a `.pt` file, it is easy to compute predictions on a new set of predictors. In this example, we will compute predictions on the test set, which was subset a few cells above. It is important to standardize the test data using the mean and standard deviation computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493b32c-73f1-480b-9243-717319b54bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights into the DeepESD architecture\n",
    "model.load_state_dict(torch.load(f'{MODELS_PATH}/{model_name}.pt'))\n",
    "\n",
    "# Standardize\n",
    "x_test_stand = deep4downscaling.trans.standardize(data_ref=x_train, data=x_test)\n",
    "\n",
    "# Compute predictions\n",
    "pred_test = deep4downscaling.deep.pred.compute_preds_standard(\n",
    "                                x_data=x_test_stand, model=model,\n",
    "                                device=device, var_target='pr',\n",
    "                                mask=y_mask, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5187291-2fce-4d27-ab65-7a9e8e80b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "deep4downscaling.viz.simple_map_plot(data=pred_test.mean('time'),\n",
    "                                     colorbar='hot_r', var_to_plot='pr',\n",
    "                                     output_path=f'./{FIGURES_PATH}/prediction_test_mean.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f76c6-4843-4ecc-91f3-7155f620d5c2",
   "metadata": {},
   "source": [
    "The `deep4downscaling.metrics` module, included within deep4downscaling, implements various metrics commonly used to assess deep learning models in the context of statistical downscaling. These include biases of different indices, spatial and probabilistic metrics, and multivariate indices, among others. In this example, we demonstrate its use by computing the relative bias of the Rx1day index between the target (test set) and the predictions for the winter months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dd949-0f99-44dc-9f96-a2635bd20d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_rel_rx1day = deep4downscaling.metrics.bias_rel_rx1day(target=y_test, pred=pred_test,\n",
    "                                                           var_target='pr', season='winter') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e06b4-4766-4928-82b8-90f723480303",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bias_rel_rx1day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
